<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Pretrain · Transformers.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="Transformers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Transformers.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../basic/">Basic</a></li><li><a class="tocitem" href="../stacks/">Stacks</a></li><li class="is-active"><a class="tocitem" href>Pretrain</a><ul class="internal"><li><a class="tocitem" href="#using-Pretrains"><span>using Pretrains</span></a></li><li><a class="tocitem" href="#API-reference"><span>API reference</span></a></li></ul></li><li><span class="tocitem">Models</span><ul><li><a class="tocitem" href="../gpt/">GPT</a></li><li><a class="tocitem" href="../bert/">BERT</a></li></ul></li><li><a class="tocitem" href="../datasets/">Datasets</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Pretrain</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Pretrain</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/chengchingwen/Transformers.jl/blob/master/docs/src/pretrain.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Transformers.Pretrain"><a class="docs-heading-anchor" href="#Transformers.Pretrain">Transformers.Pretrain</a><a id="Transformers.Pretrain-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers.Pretrain" title="Permalink"></a></h1><p>Functions for download and loading pretrain models.</p><h2 id="using-Pretrains"><a class="docs-heading-anchor" href="#using-Pretrains">using Pretrains</a><a id="using-Pretrains-1"></a><a class="docs-heading-anchor-permalink" href="#using-Pretrains" title="Permalink"></a></h2><p>For GPT and BERT, we provide a simple api to get the released pretrain weight and load them into our Julia version Transformer implementation. </p><pre><code class="language-julia hljs">using Transformers
using Transformers.Pretrain
using Transformers.GenerativePreTrain
using Transformers.BidirectionalEncoder

# disable cli download check
ENV[&quot;DATADEPS_ALWAYS_ACCEPT&quot;] = true

#load everything in the pretrain model
bert_model, wordpiece, tokenizer = pretrain&quot;Bert-uncased_L-12_H-768_A-12&quot;

#load model weight only
gpt_model = pretrain&quot;gpt-OpenAIftlm:gpt_model&quot;

#show the loaded model
show(bert_model)
show(gpt_model)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Total: 441026658 bytes
Downloaded: 4419136 bytes (1%) 8830976 bytes (2%) 13238272 bytes (3%) 17645568 bytes (4%) 22052864 bytes (5%) 26476544 bytes (6%) 30883840 bytes (7%) 35291136 bytes (8%) 39698432 bytes (9%) 44104944 bytes (10%) 48513024 bytes (11%) 52936704 bytes (12%) 57344000 bytes (13%) 61751296 bytes (14%) 66157725 bytes (15%) 70565021 bytes (16%) 74989568 bytes (17%) 79395925 bytes (18%) 83796701 bytes (19%) 88220381 bytes (20%) 92627677 bytes (21%) 97026720 bytes (22%) 101450400 bytes (23%) 105862708 bytes (24%) 110269316 bytes (25%) 114675633 bytes (26%) 119081921 bytes (27%) 123489217 bytes (28%) 127912113 bytes (29%) 132308076 bytes (30%) 136731756 bytes (31%) 141138302 bytes (32%) 145545598 bytes (33%) 149952788 bytes (34%) 154360084 bytes (35%) 158783764 bytes (36%) 163190610 bytes (37%) 167596990 bytes (38%) 172004286 bytes (39%) 176411582 bytes (40%) 180835872 bytes (41%) 185242287 bytes (42%) 189648594 bytes (43%) 194065084 bytes (44%) 198464152 bytes (45%) 202887832 bytes (46%) 207298611 bytes (47%) 211705907 bytes (48%) 216113203 bytes (49%) 220520499 bytes (50%) 224927074 bytes (51%) 229342529 bytes (52%) 233749825 bytes (53%) 238155776 bytes (54%) 242579456 bytes (55%) 246986001 bytes (56%) 251392576 bytes (57%) 255799073 bytes (58%) 260208298 bytes (59%) 264631978 bytes (60%) 269038269 bytes (61%) 273445253 bytes (62%) 277852549 bytes (63%) 282258943 bytes (64%) 286682623 bytes (65%) 291089705 bytes (66%) 295502977 bytes (67%) 299899606 bytes (68%) 304322262 bytes (69%) 308727591 bytes (70%) 313134887 bytes (71%) 317541194 bytes (72%) 321960696 bytes (73%) 326367992 bytes (74%) 330775288 bytes (75%) 335181559 bytes (76%) 339605239 bytes (77%) 344012535 bytes (78%) 348418048 bytes (79%) 352828416 bytes (80%) 357232680 bytes (81%) 361656360 bytes (82%) 366063308 bytes (83%) 370470181 bytes (84%) 374877477 bytes (85%) 379284082 bytes (86%) 383707762 bytes (87%) 388114362 bytes (88%) 392521658 bytes (89%) 396928954 bytes (90%) 401336250 bytes (91%) 405759930 bytes (92%) 410166495 bytes (93%) 414578559 bytes (94%) 418985129 bytes (95%) 423393151 bytes (96%) 427799446 bytes (97%) 432222154 bytes (98%) 436629450 bytes (99%) 441026658 bytes (100%)
[ Info: loading pretrain bert model: uncased_L-12_H-768_A-12.tfbson
Total: 469864010 bytes
Downloaded: 4702208 bytes (1%) 9404416 bytes (2%) 14106624 bytes (3%) 18808832 bytes (4%) 23494656 bytes (5%) 28196864 bytes (6%) 32899072 bytes (7%) 37600935 bytes (8%) 42303143 bytes (9%) 46988967 bytes (10%) 51691134 bytes (11%) 56393342 bytes (12%) 61095550 bytes (13%) 65781374 bytes (14%) 70483582 bytes (15%) 75185790 bytes (16%) 79887998 bytes (17%) 84590206 bytes (18%) 89276030 bytes (19%) 93978238 bytes (20%) 98680446 bytes (21%) 103382654 bytes (22%) 108084588 bytes (23%) 112776758 bytes (24%) 117478966 bytes (25%) 122180608 bytes (26%) 126865663 bytes (27%) 131567616 bytes (28%) 136269824 bytes (29%) 140972021 bytes (30%) 145657845 bytes (31%) 150360053 bytes (32%) 155062261 bytes (33%) 159764469 bytes (34%) 164466677 bytes (35%) 169152501 bytes (36%) 173854709 bytes (37%) 178551161 bytes (38%) 183253150 bytes (39%) 187955358 bytes (40%) 192657566 bytes (41%) 197343390 bytes (42%) 202045598 bytes (43%) 206747806 bytes (44%) 211440126 bytes (45%) 216146571 bytes (46%) 220848131 bytes (47%) 225535978 bytes (48%) 230237808 bytes (49%) 234940016 bytes (50%) 239631124 bytes (51%) 244333332 bytes (52%) 249043033 bytes (53%) 253739008 bytes (54%) 258441216 bytes (55%) 263127040 bytes (56%) 267829248 bytes (57%) 272531456 bytes (58%) 277233664 bytes (59%) 281919488 bytes (60%) 286621696 bytes (61%) 291323904 bytes (62%) 296026112 bytes (63%) 300728320 bytes (64%) 305414144 bytes (65%) 310116352 bytes (66%) 314818560 bytes (67%) 319520768 bytes (68%) 324206592 bytes (69%) 328908800 bytes (70%) 333611008 bytes (71%) 338313216 bytes (72%) 343015424 bytes (73%) 347701248 bytes (74%) 352403456 bytes (75%) 357105664 bytes (76%) 361807872 bytes (77%) 366510080 bytes (78%) 371195447 bytes (79%) 375897655 bytes (80%) 380599863 bytes (81%) 385301402 bytes (82%) 389987226 bytes (83%) 394690560 bytes (84%) 399392768 bytes (85%) 404094976 bytes (86%) 408797184 bytes (87%) 413483008 bytes (88%) 418185216 bytes (89%) 422887424 bytes (90%) 427589632 bytes (91%) 432275456 bytes (92%) 436977664 bytes (93%) 441679872 bytes (94%) 446382080 bytes (95%) 451084288 bytes (96%) 455770112 bytes (97%) 460472320 bytes (98%) 465174528 bytes (99%) 469864010 bytes (100%)
[ Info: loading pretrain gpt model: OpenAIftlm.npbson gpt_model
TransformerModel{Bert{Stack{NTuple{12, Transformer{MultiheadAttention{Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Flux.Dropout{Float64, Colon}}, Flux.LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, Transformers.Basic.PwFFN{Flux.Dense{typeof(NNlib.gelu), Matrix{Float32}, Vector{Float32}}, Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}, Flux.LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, Flux.Dropout{Float64, Colon}}}, Symbol(&quot;((x, m) =&gt; x&#39;:(x, m)) =&gt; 12&quot;)}, Flux.Dropout{Float64, Colon}}}(
  embed = CompositeEmbedding(tok = Embed(768), segment = Embed(768), pe = PositionEmbedding(768, max_len=512), postprocessor = Positionwise(LayerNorm((768,)), Dropout(0.1))),
  transformers = Bert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),
  classifier =
    (
      pooler =&gt; Dense(768, 768, tanh)
      masklm =&gt; (
        transform =&gt; Chain(Dense(768, 768, gelu), LayerNorm((768,)))
        output_bias =&gt; Vector{Float32}
      )
      nextsentence =&gt; Chain(Dense(768, 2), logsoftmax)
    )
)TransformerModel{Gpt{Stack{NTuple{12, Transformer{MultiheadAttention{Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Flux.Dropout{Float64, Colon}}, Flux.LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, Transformers.Basic.PwFFN{Flux.Dense{typeof(NNlib.gelu), Matrix{Float32}, Vector{Float32}}, Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}, Flux.LayerNorm{typeof(identity), Flux.Diagonal{Vector{Float32}}, Float32, 1}, Flux.Dropout{Float64, Colon}}}, Symbol(&quot;x&#39;:x =&gt; 12&quot;)}, Flux.Dropout{Float64, Colon}}}(
  embed = CompositeEmbedding(tok = Embed(768), pe = PositionEmbedding(768, max_len=512)),
  transformers = Gpt(layers=12, head=12, head_size=64, pwffn_size=3072, size=768)
)</code></pre><p>The <code>pretrain&quot;&lt;type&gt;-&lt;model-name&gt;:&lt;item&gt;&quot;</code> string with <code>pretrain</code> prefix will load the specific item from a known pretrain file (see the list below).  The <code>&lt;type&gt;</code> is matched case insensitively, so not matter <code>bert</code>, <code>Bert</code>, <code>BERT</code>, or even <code>bErT</code> will find the BERT pretrain model. On the other hand,  the <code>&lt;model-name&gt;</code>, and <code>&lt;item&gt;</code> should be exactly the one on the list. See <code>example</code>.</p><p>Currently support pretrain:</p><table><tr><th style="text-align: left">Type</th><th style="text-align: left">model</th><th style="text-align: left">model name</th><th style="text-align: left">support items</th><th style="text-align: left">detail description</th></tr><tr><td style="text-align: left">Gpt</td><td style="text-align: left">gpt</td><td style="text-align: left">OpenAIftlm</td><td style="text-align: left">gpt_model, bpe, vocab, tokenizer</td><td style="text-align: left">gpt model with 12 layers, 768 hidden units, and 12 attention heads.
subword with 40000-vocabularies-sized uncased bpe tokenizer.</td></tr><tr><td style="text-align: left">Bert</td><td style="text-align: left">scibert</td><td style="text-align: left">scibert_scivocab_uncased</td><td style="text-align: left">bert_model, wordpiece, tokenizer</td><td style="text-align: left">bert model with 12 layers, 768 hidden units, and 12 attention heads.
subword with google wordpiece uncased tokenizer.
vocabularies from scientific corpus.</td></tr><tr><td style="text-align: left">Bert</td><td style="text-align: left">scibert</td><td style="text-align: left">scibert_basevocab_cased</td><td style="text-align: left">bert_model, wordpiece, tokenizer</td><td style="text-align: left">bert model with 12 layers, 768 hidden units, and 12 attention heads.
subword with google wordpiece cased tokenizer.</td></tr><tr><td style="text-align: left">Bert</td><td style="text-align: left">scibert</td><td style="text-align: left">scibert_basevocab_uncased</td><td style="text-align: left">bert_model, wordpiece, tokenizer</td><td style="text-align: left">bert model with 12 layers, 768 hidden units, and 12 attention heads.
subword with google wordpiece uncased tokenizer.</td></tr><tr><td style="text-align: left">Bert</td><td style="text-align: left">scibert</td><td style="text-align: left">scibert_scivocab_cased</td><td style="text-align: left">bert_model, wordpiece, tokenizer</td><td style="text-align: left">bert model with 12 layers, 768 hidden units, and 12 attention heads.
subword with google wordpiece cased tokenizer.
vocabularies from scientific corpus.</td></tr><tr><td style="text-align: left">Bert</td><td style="text-align: left">bert</td><td style="text-align: left">cased_L-12_H-768_A-12</td><td style="text-align: left">bert_model, wordpiece, tokenizer</td><td style="text-align: left">bert model with 12 layers, 768 hidden units, and 12 attention heads.
subword with google wordpiece cased tokenizer.</td></tr><tr><td style="text-align: left">Bert</td><td style="text-align: left">bert</td><td style="text-align: left">wwm_cased_L-24_H-1024_A-16</td><td style="text-align: left">bert_model, wordpiece, tokenizer</td><td style="text-align: left">bert model with 24 layers, 1024 hidden units, and 16 attention heads.
subword with google wordpiece cased tokenizer.
trained with whole word masked.</td></tr><tr><td style="text-align: left">Bert</td><td style="text-align: left">bert</td><td style="text-align: left">uncased_L-12_H-768_A-12</td><td style="text-align: left">bert_model, wordpiece, tokenizer</td><td style="text-align: left">bert model with 12 layers, 768 hidden units, and 12 attention heads.
subword with google wordpiece uncased tokenizer.</td></tr><tr><td style="text-align: left">Bert</td><td style="text-align: left">bert</td><td style="text-align: left">multi_cased_L-12_H-768_A-12</td><td style="text-align: left">bert_model, wordpiece, tokenizer</td><td style="text-align: left">bert model with 12 layers, 768 hidden units, and 12 attention heads.
subword with google wordpiece multi_cased tokenizer.</td></tr><tr><td style="text-align: left">Bert</td><td style="text-align: left">bert</td><td style="text-align: left">wwm_uncased_L-24_H-1024_A-16</td><td style="text-align: left">bert_model, wordpiece, tokenizer</td><td style="text-align: left">bert model with 24 layers, 1024 hidden units, and 16 attention heads.
subword with google wordpiece cased tokenizer.
trained with whole word masked.</td></tr><tr><td style="text-align: left">Bert</td><td style="text-align: left">bert</td><td style="text-align: left">multilingual_L-12_H-768_A-12</td><td style="text-align: left">bert_model, wordpiece, tokenizer</td><td style="text-align: left">bert model with 12 layers, 768 hidden units, and 12 attention heads.
subword with google wordpiece multilingual tokenizer.</td></tr><tr><td style="text-align: left">Bert</td><td style="text-align: left">bert</td><td style="text-align: left">chinese_L-12_H-768_A-12</td><td style="text-align: left">bert_model, wordpiece, tokenizer</td><td style="text-align: left">bert model with 12 layers, 768 hidden units, and 12 attention heads.
subword with google wordpiece chinese tokenizer.</td></tr><tr><td style="text-align: left">Bert</td><td style="text-align: left">bert</td><td style="text-align: left">cased_L-24_H-1024_A-16</td><td style="text-align: left">bert_model, wordpiece, tokenizer</td><td style="text-align: left">bert model with 24 layers, 1024 hidden units, and 16 attention heads.
subword with google wordpiece cased tokenizer.</td></tr><tr><td style="text-align: left">Bert</td><td style="text-align: left">bert</td><td style="text-align: left">uncased_L-24_H-1024_A-16</td><td style="text-align: left">bert_model, wordpiece, tokenizer</td><td style="text-align: left">bert model with 24 layers, 1024 hidden units, and 16 attention heads.
subword with google wordpiece uncased tokenizer.</td></tr></table><p>If you don&#39;t find a public pretrain you want on the list, please fire an issue.</p><p>See <code>example</code> folder for the complete example.</p><h2 id="API-reference"><a class="docs-heading-anchor" href="#API-reference">API reference</a><a id="API-reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-reference" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Transformers.Pretrain.load_pretrain-Tuple{Any}" href="#Transformers.Pretrain.load_pretrain-Tuple{Any}"><code>Transformers.Pretrain.load_pretrain</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">load_pretrain(name; kw...)</code></pre><p>same as <code>@pretrain_str</code>, but can pass keyword argument if needed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/7b500dfa396ec2f5c05b875f6098d236aa9dd627/src/pretrain/Pretrain.jl#L51-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Pretrain.pretrains" href="#Transformers.Pretrain.pretrains"><code>Transformers.Pretrain.pretrains</code></a> — <span class="docstring-category">Function</span></header><section><div><p>pretrains(query::String = &quot;&quot;; detailed::Bool = false)</p><p>Show all available models. you can also query a specific <code>model</code> or <code>model name</code>. show more detail with <code>detailed = true</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/7b500dfa396ec2f5c05b875f6098d236aa9dd627/src/pretrain/config.jl#L71-L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Pretrain.@pretrain_str-Tuple{Any}" href="#Transformers.Pretrain.@pretrain_str-Tuple{Any}"><code>Transformers.Pretrain.@pretrain_str</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">pretrain&quot;model-description:item&quot;</code></pre><p>convenient macro for loading data from pretrain. Use DataDeps to automatically download if a model is not found. the string should be in <code>pretrain&quot;&lt;type&gt;-&lt;model-name&gt;:&lt;item&gt;&quot;</code> format.</p><p>see also <code>Pretrain.pretrains()</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/7b500dfa396ec2f5c05b875f6098d236aa9dd627/src/pretrain/Pretrain.jl#L32-L39">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../stacks/">« Stacks</a><a class="docs-footer-nextpage" href="../gpt/">GPT »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Saturday 13 November 2021 14:33">Saturday 13 November 2021</span>. Using Julia version 1.6.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
